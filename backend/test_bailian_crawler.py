"""
ä½¿ç”¨ä¼˜åŒ–çš„ BrowserSessionManager çˆ¬å–é˜¿é‡Œäº‘ç™¾ç‚¼æ¨¡å‹ä»·æ ¼ä¿¡æ¯

ä¼˜åŒ–ç‰¹æ€§:
1. ä¼šè¯å¤ç”¨ - æ”¯æŒå¤ç”¨å·²åˆ›å»ºçš„AgentGo session
2. é‡è¯•æœºåˆ¶ - è‡ªåŠ¨é‡è¯•å¤±è´¥çš„è¿æ¥
3. è‡ªåŠ¨Fallback - AgentGoä¸å¯ç”¨æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°æœ¬åœ°æµè§ˆå™¨
4. èµ„æºæ¸…ç† - è‡ªåŠ¨é‡Šæ”¾sessionèµ„æº

æ–‡æ¡£: https://docs.agentgo.live/fundamentals/using-browser-session
"""
import asyncio
import json
import os
import re
from typing import List, Dict, Any, Optional

from browser_session_manager import (
    BrowserSessionManager,
    SessionConfig,
    SessionMode
)

# ç›®æ ‡ URL - é˜¿é‡Œäº‘ç™¾ç‚¼æ¨¡å‹ä»·æ ¼é¡µé¢ (ä½¿ç”¨å…¬å¼€å¸®åŠ©æ–‡æ¡£)
TARGET_URL = "https://help.aliyun.com/zh/model-studio/getting-started/models"

# è¾“å‡ºç›®å½•
OUTPUT_DIR = os.path.dirname(os.path.abspath(__file__))


def get_session_config() -> SessionConfig:
    """
    è·å–ä¼šè¯é…ç½®
    
    ä¼˜å…ˆä½¿ç”¨AgentGoï¼Œå¦‚æœAPI Keyæœªé…ç½®æˆ–è¿æ¥å¤±è´¥ï¼Œè‡ªåŠ¨fallbackåˆ°æœ¬åœ°æµè§ˆå™¨
    """
    api_key = os.getenv("AGENTGO_API_KEY", "")
    
    return SessionConfig(
        api_key=api_key,
        region="sg",  # æ–°åŠ å¡åŒºåŸŸï¼Œå¯¹ä¸­å›½ç«™ç‚¹å»¶è¿Ÿè¾ƒä½
        disable_proxy=False,
        connection_timeout=30000,
        page_load_timeout=90000,  # å¢åŠ åˆ°90ç§’ï¼Œé˜¿é‡Œäº‘æ–‡æ¡£ç«™åŠ è½½è¾ƒæ…¢
        max_retries=3,
        retry_delay=3.0,  # å¢åŠ é‡è¯•é—´éš”
        headless=True,
        mode=SessionMode.LOCAL  # ç›´æ¥ä½¿ç”¨æœ¬åœ°æµè§ˆå™¨ï¼ˆAgentGo API Keyå·²è¿‡æœŸï¼‰
    )


async def crawl_bailian_models() -> Optional[List[Dict[str, Any]]]:
    """
    çˆ¬å–ç™¾ç‚¼æ¨¡å‹ä»·æ ¼ä¿¡æ¯
    
    Returns:
        æ¨¡å‹ä¿¡æ¯åˆ—è¡¨ï¼Œå¤±è´¥è¿”å›None
    """
    print("=" * 70)
    print("ğŸš€ å¼€å§‹çˆ¬å–é˜¿é‡Œäº‘ç™¾ç‚¼æ¨¡å‹ä»·æ ¼ä¿¡æ¯")
    print("=" * 70)
    
    config = get_session_config()
    
    # æ˜¾ç¤ºå½“å‰é…ç½®
    if config.api_key:
        print(f"\nğŸ“¡ é…ç½®æ¨¡å¼: AUTO (ä¼˜å…ˆAgentGo, region={config.region})")
    else:
        print(f"\nğŸ“¡ é…ç½®æ¨¡å¼: LOCAL (æœªé…ç½®AgentGo API Key)")
    
    async with BrowserSessionManager(config) as manager:
        try:
            # ä½¿ç”¨ get_page ä¸Šä¸‹æ–‡ç®¡ç†å™¨çˆ¬å–
            async with manager.get_page(TARGET_URL, wait_until="domcontentloaded") as page:
                # ç­‰å¾…è¡¨æ ¼åŠ è½½
                try:
                    await page.wait_for_selector("table", timeout=30000)
                    print("âœ… æ£€æµ‹åˆ°è¡¨æ ¼å…ƒç´ ")
                except Exception:
                    print("âš ï¸ æœªæ£€æµ‹åˆ°è¡¨æ ¼ï¼Œç»§ç»­å¤„ç†...")
                
                # æ»šåŠ¨åŠ è½½åŠ¨æ€å†…å®¹
                await scroll_to_load_all(page, manager)
                
                # æå–æ¨¡å‹ä¿¡æ¯
                models = await extract_model_info(page)
                
                # æ‰“å°ä¼šè¯ç»Ÿè®¡
                stats = manager.get_session_stats()
                print(f"\nğŸ“Š ä¼šè¯ç»Ÿè®¡: mode={stats['mode']}, sessions={stats['total_sessions']}")
                
                return models
            
        except Exception as e:
            print(f"âŒ çˆ¬å–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None


async def scroll_to_load_all(page, manager: BrowserSessionManager):
    """
    æ»šåŠ¨é¡µé¢åŠ è½½æ‰€æœ‰åŠ¨æ€å†…å®¹
    """
    print("â³ æ»šåŠ¨åŠ è½½åŠ¨æ€å†…å®¹...")
    await manager.scroll_and_load(page, scroll_pause=1.0, max_scrolls=5)
    
    # ä¿å­˜HTMLå’Œæˆªå›¾ç”¨äºè°ƒè¯•
    html_path = os.path.join(OUTPUT_DIR, "bailian_page.html")
    screenshot_path = os.path.join(OUTPUT_DIR, "bailian_screenshot.png")
    
    await manager.save_html(page, html_path)
    await manager.take_screenshot(page, screenshot_path)


async def extract_model_info(page) -> list:
    """
    ä»é¡µé¢ä¸­æå–æ¨¡å‹ä¿¡æ¯
    """
    models = []
    
    print("\nğŸ“Š å¼€å§‹æå–æ¨¡å‹ä¿¡æ¯...")
    
    # å°è¯•è·å–æ‰€æœ‰è¡¨æ ¼
    tables = await page.query_selector_all("table")
    print(f"   æ‰¾åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
    
    for i, table in enumerate(tables):
        try:
            # è·å–è¡¨å¤´
            headers = await table.query_selector_all("th")
            header_texts = [await h.inner_text() for h in headers]
            
            # è·å–è¡¨æ ¼è¡Œ
            rows = await table.query_selector_all("tbody tr")
            
            print(f"\n   è¡¨æ ¼ {i+1}: {len(rows)} è¡Œæ•°æ®")
            if header_texts:
                print(f"   è¡¨å¤´: {header_texts[:5]}...")  # åªæ˜¾ç¤ºå‰5ä¸ª
            
            for row in rows:
                cells = await row.query_selector_all("td")
                cell_texts = [await c.inner_text() for c in cells]
                
                if cell_texts and len(cell_texts) > 0:
                    model_info = {
                        "raw_data": cell_texts,
                        "table_index": i
                    }
                    
                    # å°è¯•è¯†åˆ«æ¨¡å‹åç§°ï¼ˆé€šå¸¸åŒ…å« qwen, deepseek ç­‰å…³é”®è¯ï¼‰
                    for text in cell_texts:
                        text_lower = text.lower()
                        if any(kw in text_lower for kw in ["qwen", "deepseek", "glm", "llama", "baichuan"]):
                            model_info["model_name"] = text.strip()
                            break
                    
                    # å°è¯•è¯†åˆ«ä»·æ ¼ï¼ˆåŒ…å«æ•°å­—å’Œå…ƒï¼‰
                    for text in cell_texts:
                        if re.search(r'\d+\.?\d*\s*(å…ƒ|Â¥|\$)', text):
                            model_info["price_info"] = text.strip()
                    
                    models.append(model_info)
                    
        except Exception as e:
            print(f"   âš ï¸ è§£æè¡¨æ ¼ {i+1} æ—¶å‡ºé”™: {e}")
    
    # å°è¯•å…¶ä»–æ–¹å¼æå–ï¼ˆå¯èƒ½æ˜¯éè¡¨æ ¼ç»“æ„ï¼‰
    if not models:
        print("\n   å°è¯•å…¶ä»–æ–¹å¼æå–...")
        
        # æŸ¥æ‰¾åŒ…å«ä»·æ ¼ä¿¡æ¯çš„å…ƒç´ 
        all_text = await page.inner_text("body")
        
        # ä½¿ç”¨æ­£åˆ™åŒ¹é…æ¨¡å‹åç§°
        model_patterns = [
            r'(qwen[\w\-\.]+)',
            r'(deepseek[\w\-\.]+)',
            r'(glm[\w\-\.]+)',
            r'(llama[\w\-\.]+)',
        ]
        
        found_models = set()
        for pattern in model_patterns:
            matches = re.findall(pattern, all_text, re.IGNORECASE)
            found_models.update(matches)
        
        for model in found_models:
            models.append({"model_name": model, "source": "text_extraction"})
    
    return models


def print_results(models: list):
    """
    æ‰“å°ç»“æœ
    """
    print("\n" + "=" * 70)
    print("ğŸ“‹ çˆ¬å–ç»“æœæ±‡æ€»")
    print("=" * 70)
    
    if not models:
        print("âŒ æœªèƒ½æå–åˆ°æ¨¡å‹ä¿¡æ¯")
        return
    
    print(f"\nâœ… å…±è¯†åˆ«åˆ° {len(models)} æ¡æ¨¡å‹ç›¸å…³æ•°æ®")
    
    # æŒ‰æ˜¯å¦æœ‰æ¨¡å‹åç§°åˆ†ç±»
    named_models = [m for m in models if m.get("model_name")]
    unnamed_data = [m for m in models if not m.get("model_name")]
    
    if named_models:
        print(f"\nğŸ“Œ è¯†åˆ«åˆ°çš„æ¨¡å‹åç§° ({len(named_models)} ä¸ª):")
        seen = set()
        for m in named_models:
            name = m.get("model_name", "").lower()
            if name not in seen:
                seen.add(name)
                price = m.get("price_info", "ä»·æ ¼å¾…è§£æ")
                print(f"   â€¢ {m['model_name']}")
                if price != "ä»·æ ¼å¾…è§£æ":
                    print(f"     ä»·æ ¼: {price}")
    
    if unnamed_data:
        print(f"\nğŸ“ å…¶ä»–æ•°æ®è¡Œ ({len(unnamed_data)} æ¡):")
        for i, m in enumerate(unnamed_data[:10]):  # åªæ˜¾ç¤ºå‰10æ¡
            raw = m.get("raw_data", [])
            if raw:
                print(f"   {i+1}. {' | '.join(str(x)[:30] for x in raw[:3])}")
        if len(unnamed_data) > 10:
            print(f"   ... è¿˜æœ‰ {len(unnamed_data) - 10} æ¡æ•°æ®")


async def main():
    """ä¸»å‡½æ•°"""
    models = await crawl_bailian_models()
    if models is not None:
        print_results(models)
    
    print("\n" + "=" * 70)
    print("âœ¨ çˆ¬å–ä»»åŠ¡å®Œæˆ")
    print("=" * 70)


if __name__ == "__main__":
    asyncio.run(main())
