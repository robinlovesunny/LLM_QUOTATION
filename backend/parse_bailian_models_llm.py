#!/usr/bin/env python3
"""
åŸºäº qwen-plus çš„ç™¾ç‚¼æ¨¡å‹æ•°æ®è§£æå™¨
ä½¿ç”¨LLMä»HTMLä¸­æå–ç»“æ„åŒ–æ¨¡å‹æ•°æ®
"""

import os
import json
import re
from typing import List, Dict, Optional
from bs4 import BeautifulSoup
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# JSON Schema å®šä¹‰
MODEL_SCHEMA = {
    "type": "object",
    "properties": {
        "model_id": {"type": "string", "description": "APIè°ƒç”¨çš„æ¨¡å‹æ ‡è¯†ç¬¦ï¼Œå¦‚qwen-plusã€deepseek-v3"},
        "model_name": {"type": "string", "description": "æ¨¡å‹æ˜¾ç¤ºåç§°"},
        "vendor": {"type": "string", "enum": ["aliyun", "deepseek", "zhipu", "baichuan", "meta", "other"]},
        "category": {"type": "string", "enum": ["text_generation", "vision", "audio", "embedding", "rerank"]},
        "specs": {
            "type": "object",
            "properties": {
                "max_context_length": {"type": "integer"},
                "max_input_tokens": {"type": "integer"},
                "max_output_tokens": {"type": "integer"},
                "max_thinking_tokens": {"type": "integer"}
            }
        },
        "pricing": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "region": {"type": "string"},
                    "input_price": {"type": "object", "properties": {"price": {"type": "number"}, "unit": {"type": "string"}}},
                    "output_price": {"type": "object", "properties": {"price": {"type": "number"}, "unit": {"type": "string"}}},
                    "thinking_input_price": {"type": "object"},
                    "thinking_output_price": {"type": "object"}
                }
            }
        }
    },
    "required": ["model_id", "model_name", "vendor", "category"]
}


class LLMModelParser:
    """ä½¿ç”¨ qwen-plus è§£ææ¨¡å‹æ•°æ®"""
    
    def __init__(self):
        self.client = OpenAI(
            api_key=os.getenv("DASHSCOPE_API_KEY"),
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        self.model = "qwen-plus"
    
    def parse_html_file(self, html_path: str) -> Dict:
        """è§£æHTMLæ–‡ä»¶"""
        print("=" * 60)
        print("ğŸ¤– å¼€å§‹ä½¿ç”¨ qwen-plus è§£æç™¾ç‚¼æ¨¡å‹æ•°æ®")
        print("=" * 60)
        
        with open(html_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # ä½¿ç”¨BeautifulSoupé¢„å¤„ç†ï¼Œæå–è¡¨æ ¼
        soup = BeautifulSoup(html_content, 'html.parser')
        tables = soup.find_all('table')
        
        print(f"\nğŸ“Š æ‰¾åˆ° {len(tables)} ä¸ªè¡¨æ ¼")
        
        # é¢„ç­›é€‰æœ‰æ•ˆçš„å®šä»·è¡¨æ ¼
        valid_tables = []
        for table in tables:
            text = self._extract_table_text(table)
            # å¿…é¡»åŒ…å«ä»·æ ¼å…³é”®è¯å’Œæ¨¡å‹ç›¸å…³è¯
            has_price = any(kw in text for kw in ['å…ƒ/', 'å…ƒ/åƒ', 'Token'])
            has_model = any(kw in text.lower() for kw in ['qwen', 'deepseek', 'llama', 'glm', 'model', 'æ¨¡å‹'])
            if has_price and has_model and len(text) > 100:
                valid_tables.append((table, text))
        
        print(f"ğŸ“‹ æœ‰æ•ˆå®šä»·è¡¨æ ¼: {len(valid_tables)} ä¸ª")
        
        all_models = []
        total_input_tokens = 0
        total_output_tokens = 0
        
        for i, (table, table_text) in enumerate(valid_tables):
            print(f"\nğŸ”„ å¤„ç†è¡¨æ ¼ {i+1}/{len(valid_tables)}...")
            
            # ä½¿ç”¨LLMè§£æ
            models, usage = self._parse_table_with_llm(table_text, i+1)
            if models:
                all_models.extend(models)
                total_input_tokens += usage.get('input', 0)
                total_output_tokens += usage.get('output', 0)
                print(f"   âœ… æå–åˆ° {len(models)} ä¸ªæ¨¡å‹")
        
        # å»é‡
        unique_models = self._deduplicate_models(all_models)
        
        result = {
            "source": "bailian_llm_parsed",
            "parser": "qwen-plus",
            "model_count": len(unique_models),
            "models": unique_models,
            "token_usage": {
                "input_tokens": total_input_tokens,
                "output_tokens": total_output_tokens,
                "total_tokens": total_input_tokens + total_output_tokens
            }
        }
        
        # ä¿å­˜ç»“æœ
        output_path = html_path.replace('.html', '_llm.json')
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… LLMè§£æå®Œæˆ!")
        print(f"   - æ¨¡å‹æ•°é‡: {len(unique_models)}")
        print(f"   - Tokenæ¶ˆè€—: è¾“å…¥={total_input_tokens}, è¾“å‡º={total_output_tokens}")
        print(f"   - è¾“å‡ºæ–‡ä»¶: {output_path}")
        
        return result
    
    def _extract_table_text(self, table) -> str:
        """å°†è¡¨æ ¼è½¬æ¢ä¸ºç»“æ„åŒ–æ–‡æœ¬"""
        rows = table.find_all('tr')
        if not rows:
            return ""
        
        lines = []
        for row in rows:
            cells = row.find_all(['th', 'td'])
            cell_texts = [cell.get_text(strip=True) for cell in cells]
            if any(cell_texts):
                lines.append(' | '.join(cell_texts))
        
        return '\n'.join(lines)
    
    def _parse_table_with_llm(self, table_text: str, table_index: int) -> tuple:
        """ä½¿ç”¨LLMè§£æè¡¨æ ¼"""
        
        # é™åˆ¶æ–‡æœ¬é•¿åº¦ï¼Œé¿å…è¶…å‡ºä¸Šä¸‹æ–‡
        if len(table_text) > 8000:
            table_text = table_text[:8000] + "\n...(æˆªæ–­)"
        
        prompt = f"""è¯·ä»ä»¥ä¸‹è¡¨æ ¼æ•°æ®ä¸­æå–LLMæ¨¡å‹ä¿¡æ¯ã€‚

è¡¨æ ¼å†…å®¹:
{table_text}

è¯·æå–æ¯ä¸ªæ¨¡å‹çš„ä»¥ä¸‹ä¿¡æ¯:
1. model_id: APIè°ƒç”¨çš„æ¨¡å‹æ ‡è¯†ç¬¦(å¦‚qwen-plus, deepseek-v3ï¼Œçº¯å­—æ¯æ•°å­—å’Œ-_.)
2. model_name: æ¨¡å‹åç§°
3. vendor: å‚å•†(aliyun/deepseek/zhipu/baichuan/meta/other)
4. category: ç±»åˆ«(text_generation/vision/audio/embedding/rerank)
5. specs: è§„æ ¼(max_context_length, max_input_tokens, max_output_tokens, max_thinking_tokens)
6. pricing: å®šä»·ä¿¡æ¯(input_price, output_priceï¼Œå•ä½æ˜¯å…ƒ/åƒToken)

é‡è¦è§„åˆ™:
- model_idå¿…é¡»æ˜¯æ ‡å‡†APIåç§°æ ¼å¼ï¼Œä¸åŒ…å«ä¸­æ–‡
- æ’é™¤å…è´¹é¢åº¦ç›¸å…³çš„ä»·æ ¼ä¿¡æ¯
- ä»·æ ¼å•ä½ç»Ÿä¸€è½¬æ¢ä¸º å…ƒ/åƒToken
- å¦‚æœæœ‰æ€è€ƒæ¨¡å¼ä»·æ ¼ï¼Œå•ç‹¬è®°å½•

è¯·ä»¥JSONæ•°ç»„æ ¼å¼è¿”å›ï¼Œåªè¿”å›JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—:
```json
[{{"model_id": "xxx", "model_name": "xxx", ...}}]
```"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ•°æ®æå–ä¸“å®¶ï¼Œæ“…é•¿ä»è¡¨æ ¼ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯ã€‚åªè¿”å›JSONæ ¼å¼æ•°æ®ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=4000
            )
            
            content = response.choices[0].message.content
            usage = {
                'input': response.usage.prompt_tokens,
                'output': response.usage.completion_tokens
            }
            
            # æå–JSON
            models = self._extract_json_from_response(content)
            return models, usage
            
        except Exception as e:
            print(f"   âŒ LLMè°ƒç”¨å¤±è´¥: {e}")
            return [], {'input': 0, 'output': 0}
    
    def _extract_json_from_response(self, content: str) -> List[Dict]:
        """ä»LLMå“åº”ä¸­æå–JSON"""
        # å°è¯•æå–ä»£ç å—ä¸­çš„JSON
        json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', content)
        if json_match:
            json_str = json_match.group(1)
        else:
            # å°è¯•ç›´æ¥è§£æ
            json_str = content
        
        try:
            data = json.loads(json_str)
            if isinstance(data, list):
                return self._normalize_models(data)
            elif isinstance(data, dict) and 'models' in data:
                return self._normalize_models(data['models'])
            return []
        except json.JSONDecodeError:
            return []
    
    def _normalize_models(self, models: List[Dict]) -> List[Dict]:
        """æ ‡å‡†åŒ–æ¨¡å‹æ•°æ®æ ¼å¼"""
        normalized = []
        for m in models:
            if not isinstance(m, dict):
                continue
            if not m.get('model_id'):
                continue
            
            # æ ‡å‡†åŒ–model_id
            model_id = str(m['model_id']).lower().strip()
            
            # æ ‡å‡†åŒ–pricing
            pricing = []
            raw_pricing = m.get('pricing')
            if raw_pricing and isinstance(raw_pricing, list):
                for p in raw_pricing:
                    if isinstance(p, dict):
                        pricing.append(self._normalize_pricing(p))
            elif isinstance(m.get('input_price'), (int, float, dict)):
                pricing.append(self._normalize_pricing(m))
            
            normalized.append({
                "model_id": model_id,
                "model_name": str(m.get('model_name', model_id)),
                "vendor": m.get('vendor', 'aliyun'),
                "category": m.get('category', 'text_generation'),
                "specs": m.get('specs') if isinstance(m.get('specs'), dict) else None,
                "pricing": pricing,
                "status": "active"
            })
        
        return normalized
    
    def _normalize_pricing(self, p: Dict) -> Dict:
        """æ ‡å‡†åŒ–ä»·æ ¼æ ¼å¼"""
        def parse_price(val):
            if isinstance(val, dict):
                return val
            if isinstance(val, (int, float)):
                return {"price": float(val), "unit": "åƒToken", "unit_quantity": 1000}
            return None
        
        return {
            "region": p.get('region', 'cn-shanghai'),
            "input_price": parse_price(p.get('input_price')),
            "output_price": parse_price(p.get('output_price')),
            "thinking_input_price": parse_price(p.get('thinking_input_price')),
            "thinking_output_price": parse_price(p.get('thinking_output_price'))
        }
    
    def _deduplicate_models(self, models: List[Dict]) -> List[Dict]:
        """å»é‡ï¼Œä¿ç•™ä¿¡æ¯æœ€å®Œæ•´çš„è®°å½•"""
        seen = {}
        for m in models:
            mid = m['model_id']
            if mid not in seen:
                seen[mid] = m
            else:
                # ä¿ç•™ä¿¡æ¯æ›´å®Œæ•´çš„
                existing = seen[mid]
                if m.get('pricing') and not existing.get('pricing'):
                    seen[mid] = m
                elif m.get('specs') and not existing.get('specs'):
                    seen[mid]['specs'] = m['specs']
        
        return list(seen.values())


def main():
    parser = LLMModelParser()
    result = parser.parse_html_file("bailian_page.html")
    
    # æ˜¾ç¤ºå‚å•†åˆ†å¸ƒ
    vendors = {}
    for m in result['models']:
        v = m.get('vendor', 'unknown')
        vendors[v] = vendors.get(v, 0) + 1
    
    print(f"\nğŸ“Š å‚å•†åˆ†å¸ƒ:")
    for v, c in sorted(vendors.items(), key=lambda x: -x[1]):
        print(f"   - {v}: {c}")
    
    # æ˜¾ç¤ºæœ‰ä»·æ ¼çš„æ¨¡å‹æ•°
    with_price = sum(1 for m in result['models'] if m.get('pricing'))
    print(f"\nğŸ“Š æœ‰ä»·æ ¼æ•°æ®: {with_price}/{len(result['models'])}")


if __name__ == "__main__":
    main()
